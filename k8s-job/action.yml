name: Run Kubernetes Job 
description: Runs a Kubernetes job and outputs the pod logs to the console.
inputs:
  file: 
    description: 'The file with a k8s job to run'
    default: job.yaml
  timeout:
    description: 'The timeout for the job'
    default: 300s
runs:
  using: composite
  steps:

  - name: Apply and Watch
    shell: bash
    env:
      JOB_FILE: ${{ inputs.file }}
      JOB_TIMEOUT: ${{ inputs.timeout }}
    run: |
      # Exporting environment variables for use within the shell script
      export JOB JOB_NAME NAMESPACE PODS
      JOB="$(cat "$JOB_FILE")"

      # Extracting relevant metadata from the Kubernetes job file
      JOB_NAME="$(echo "$JOB" | yq -r '.metadata.name')"
      NAMESPACE="$(echo "$JOB" | yq -r '.metadata.namespace')"

      # Function to output pod logs
      function pod_logs() {
        wait_for_resource "pods/${1}" && \
        kubectl wait \
          --for=condition=Ready \
          -n "$NAMESPACE" \
          --timeout="${JOB_TIMEOUT}" \
          "pods/${1}" && \
        # If the pod is placed on a node that just started (e.g. because the job triggered a scale up) and we try to
        # get logs too quickly, sometimes we get TLS errors from the node. Waiting for the node to be in the "Ready" state
        # doesn't reliably resolve this, and also can't be done with tenant-scoped permissions.
        retries=5
        while ! kubectl logs "pods/$1" -n "$NAMESPACE" --follow; do
            retries=$((retries-1))
            if [ "$retries" == "0" ]; then
                break
            fi
            echo "Retrying..."
            sleep 5
        done
      }

      # Function to wait for the existence of a Kubernetes resource
      function wait_for_resource() {
        while : ; do
          res="$(kubectl get "$1" -n "$NAMESPACE" 2>&1)" && break
          sleep 2
        done
      }

      # Apply the Kubernetes job
      kubectl apply -f "$JOB_FILE"

      # Wait for the Kubernetes job resource to exist
      wait_for_resource "jobs/${JOB_NAME}"

      # Get the list of pods created by the job
      # shellcheck disable=SC2207
      PODS=($(kubectl get pods \
        -l job-name="${JOB_NAME}" \
        -n "$NAMESPACE" \
        --no-headers \
        --output=custom-columns=:metadata.name))

      length="${#PODS[@]}"

      # Wait for the Kubernetes job to complete
      echo "Waiting for ${JOB_NAME} to complete and ${length} pods to be ready"
      kubectl wait \
        --for=condition=Complete \
        --timeout="${JOB_TIMEOUT}" \
        -n "$NAMESPACE" \
        "jobs/${JOB_NAME}" &

      # Output logs for each pod in parallel
      for pod in "${PODS[@]}"; do
        pod_logs "$pod" &
      done

      # Wait for all background processes to complete
      wait

      job_status=$(kubectl get job "${JOB_NAME}" -n "${NAMESPACE}" -o=jsonpath='{.status.conditions[?(@.type=="Failed")].status}')

      # Delete the Kubernetes job (cleanup)
      kubectl delete -f "$JOB_FILE"

      # job_status update
      if [ "${job_status}" == "True" ]; then
          echo "Error: Kubernetes job ${JOB_NAME} failed or timed out"
          exit 1
      else
          echo "Executed ${length} pods for job ${JOB_NAME}"
      fi
